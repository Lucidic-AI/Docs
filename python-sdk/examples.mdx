---
title: 'Examples'
description: 'Complete examples showing manual control and advanced features'
---

## Overview

This page contains detailed examples showing how to use Lucidic's Python SDK with full control over sessions and events. While the [quick start](/getting-started/quickstart) handles everything automatically, these examples demonstrate manual control for complex workflows.

## Basic Session with Events

```python
from lucidicai import LucidicAI
from openai import OpenAI

# Initialize clients
client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

# Use context manager for session lifecycle
with client.sessions.create(session_name="wiki_search", task="Search for Stanford University"):
    # LLM calls are automatically tracked as events
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Tell me about Stanford University"}]
    )

    # Create manual events for non-LLM operations
    client.events.emit(
        name="Navigate to search",
        input={"action": "navigation"},
        output={"result": "Reached search page"}
    )

    # Another LLM call (auto-tracked)
    summary = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Summarize the findings"}]
    )

# Session auto-ends when context exits
```

## Working with Events

### Zero-Latency Event Emission

Use `emit()` for hot paths where latency matters:

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])

with client.sessions.create(session_name="High Performance Task"):
    # Zero-latency - doesn't block execution
    client.events.emit(
        name="Cache lookup",
        input={"key": "user_123"},
        output={"hit": True, "value": {"name": "John"}}
    )

    # Continue immediately while event is sent in background
    process_user_data()
```

### Synchronous Event Creation

Use `create()` when you need confirmation or the event ID:

```python
with client.sessions.create(session_name="Document Processing"):
    # Synchronous - waits for confirmation
    event = client.events.create(
        name="Database query",
        input={"query": "SELECT * FROM users WHERE active = true"},
        output={"count": 150},
        duration_ms=45
    )

    print(f"Event created with ID: {event.event_id}")
```

### Async Event Creation

For async workflows:

```python
import asyncio
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])

async def process_documents():
    async with client.sessions.acreate(session_name="Async Processing"):
        # Async event creation
        event = await client.events.acreate(
            name="Async document fetch",
            input={"doc_id": "doc_123"},
            output={"status": "fetched"}
        )

        # Process document
        await process_doc()

asyncio.run(process_documents())
```

## Using Decorators

Decorators provide a clean way to track function calls as events:

### Basic Decorator Usage

```python
from lucidicai import LucidicAI
from openai import OpenAI

client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

@client.event()
def validate_request(request: dict) -> bool:
    """Validation logic - automatically tracked as an event."""
    return "user_id" in request and "message" in request

@client.event()
def process_request(request: dict) -> dict:
    """Processing logic - automatically tracked as an event."""
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": request["message"]}]
    )
    return {"status": "success", "response": response.choices[0].message.content}

# Usage
with client.sessions.create(session_name="Request Handler"):
    request = {"user_id": "123", "message": "Hello!"}

    if validate_request(request):
        result = process_request(request)
        print(result)

# Event hierarchy:
# Session
# ├── Event: validate_request (Function Call)
# └── Event: process_request (Function Call)
#     └── Event: GPT-4 call (LLM Generation)
```

### Decorators with Async Functions

```python
@client.event()
async def fetch_data(url: str) -> dict:
    """Async function - automatically tracked."""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.json()

@client.event()
async def async_workflow(urls: list) -> dict:
    """Orchestrates multiple async fetches."""
    results = []
    for url in urls:
        data = await fetch_data(url)
        results.append(data)
    return {"fetched": len(results), "data": results}
```

### Nested Decorated Functions

```python
@client.event()
def parent_workflow(items: list) -> dict:
    """Parent function that orchestrates child functions."""
    results = []

    for item in items:
        result = process_item(item)
        results.append(result)

    summary = summarize_results(results)
    return {"processed": len(results), "summary": summary}

@client.event()
def process_item(item: dict) -> dict:
    """Child function for each item."""
    validated = validate_item(item)
    transformed = transform_item(validated)
    return transformed

@client.event()
def validate_item(item: dict) -> dict:
    return item

@client.event()
def transform_item(item: dict) -> dict:
    return {"transformed": item}

@client.event()
def summarize_results(results: list) -> str:
    return f"Processed {len(results)} items successfully"

# Results in nested event hierarchy:
# Session
# └── Event: parent_workflow
#     ├── Event: process_item
#     │   ├── Event: validate_item
#     │   └── Event: transform_item
#     ├── Event: process_item
#     │   ├── Event: validate_item
#     │   └── Event: transform_item
#     └── Event: summarize_results
```

## Authentication Examples

### Using Environment Variables (Recommended)

```python
# Set these in your environment or .env file:
# LUCIDIC_API_KEY=your-api-key
# LUCIDIC_AGENT_ID=your-agent-id

from lucidicai import LucidicAI

client = LucidicAI(providers=["openai"])  # Credentials loaded automatically

with client.sessions.create(session_name="My Session"):
    # Your workflow
    pass
```

### Direct Parameters

```python
from lucidicai import LucidicAI

client = LucidicAI(
    api_key="your-api-key",
    agent_id="your-agent-id",
    providers=["openai"]
)

with client.sessions.create(session_name="My Session"):
    # Your workflow
    pass
```

### Using dotenv

```python
from dotenv import load_dotenv
from lucidicai import LucidicAI

load_dotenv()  # Load from .env file

client = LucidicAI(providers=["anthropic"])

with client.sessions.create(session_name="My Session"):
    # Your workflow
    pass
```

## Using Prompt Database

```python
from lucidicai import LucidicAI
from openai import OpenAI

client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

with client.sessions.create(session_name="Prompt DB Example"):
    # Pull prompt from Prompt DB
    prompt = client.prompts.get(
        prompt_name="summarize_webpage",
        variables={
            "page_text": "Stanford University is a private research university...",
            "max_length": "100 words"
        },
        label="production",  # Use production version
        cache_ttl=60  # Cache for 60 seconds
    )

    # Use the prompt in your LLM call
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
```

## Experiment Usage

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])

# Experiments must be created in the dashboard first
# The experiment ID can be found in the URL after creation
EXPERIMENT_ID = "exp-123abc-456def"

# Add sessions to the experiment
with client.sessions.create(
    session_name="Performance Test 1",
    experiment_id=EXPERIMENT_ID,
    task="Test checkout flow performance"
):
    # Your agent workflow here
    process_checkout()

# Session automatically ends with the context manager
```

## Dataset Testing

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])

# When running dataset tests, link sessions to dataset items
dataset_items = client.datasets.get_items(dataset_id="test_suite_v1")

for item in dataset_items:
    with client.sessions.create(
        session_name=f"Test: {item['name']}",
        dataset_item_id=item['id']
    ):
        # Run your agent with the test input
        result = my_agent.process(item['input'])

        # Validate against expected output
        success = result == item['expected_output']

        # Update session with success status
        client.sessions.update(is_successful=success)
```

## Production Monitoring

```python
from lucidicai import LucidicAI

client = LucidicAI(
    api_key="...",
    providers=["openai", "anthropic"],
    production_monitoring=True  # Optimized for production
)

def mask_pii(text):
    # Custom logic to mask sensitive information
    return text.replace("SSN:", "XXX-XX-")

with client.sessions.create(
    session_name="Production Agent",
    masking_function=mask_pii,
    tags=["production", "v2.0", "customer-service"]
):
    # Your production workflow
    process_user_request()

# Session automatically ends and data is sent efficiently
```

## Error Handling

```python
from lucidicai import LucidicAI
from lucidicai.errors import (
    APIKeyVerificationError,
    InvalidOperationError,
    LucidicNotInitializedError
)

try:
    client = LucidicAI(api_key="...", providers=["openai"])

    with client.sessions.create(session_name="My Session"):
        try:
            # Your agent logic that might fail
            process_data()
        except Exception as e:
            # Emit error event
            client.events.emit(
                name="Processing error",
                input={"operation": "process_data"},
                output={"error": str(e)},
                metadata={"error_type": type(e).__name__}
            )
            raise

except APIKeyVerificationError:
    print("Invalid API credentials. Check your keys.")
except InvalidOperationError as e:
    print(f"Operation error: {e}")
```

## Complete RAG Pipeline

```python
from lucidicai import LucidicAI
from openai import OpenAI

# Initialize clients
client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

@client.event()
def search_documents(query: str) -> list:
    """Search for relevant documents."""
    return database.search(query, limit=5)

@client.event()
def generate_response(query: str, context: list) -> str:
    """Generate a response using the LLM."""
    messages = [
        {"role": "system", "content": "Answer based on the context provided."},
        {"role": "user", "content": f"Context: {context}\n\nQuestion: {query}"}
    ]

    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=messages
    )
    # LLM call is nested inside this event
    return response.choices[0].message.content

@client.event()
def rag_pipeline(query: str) -> str:
    """Complete RAG pipeline."""
    documents = search_documents(query)
    response = generate_response(query, documents)
    return response

# Run the pipeline
with client.sessions.create(session_name="RAG Query"):
    answer = rag_pipeline("What is machine learning?")
    print(answer)

# Event hierarchy:
# Session
# └── Event: rag_pipeline (Function Call)
#     ├── Event: search_documents (Function Call)
#     └── Event: generate_response (Function Call)
#         └── Event: GPT-4 call (LLM Generation)
```

## Multi-Step Research Assistant

```python
from lucidicai import LucidicAI
from openai import OpenAI

# Initialize
client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

@client.event(name="Search arXiv papers")
def search_papers(query: str, limit: int = 5) -> list:
    """Search arXiv for papers."""
    return search_arxiv(query, limit=limit)

@client.event(name="Analyze paper")
def analyze_paper(paper: dict) -> dict:
    """Analyze a single paper using LLM."""
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{
            "role": "user",
            "content": f"Summarize this paper: {paper['abstract']}"
        }]
    )
    return {
        "title": paper["title"],
        "summary": response.choices[0].message.content
    }

@client.event(name="Generate final report")
def create_report(analyses: list) -> str:
    """Create a comprehensive report from all analyses."""
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{
            "role": "user",
            "content": f"Create a comprehensive report from these summaries: {analyses}"
        }]
    )
    return response.choices[0].message.content

# Run the research workflow
with client.sessions.create(
    session_name="research_assistant",
    task="Research and summarize recent AI developments"
):
    # Phase 1: Search
    papers = search_papers("artificial intelligence", limit=5)

    # Emit progress event
    client.events.emit(
        name="Search complete",
        output={"papers_found": len(papers)}
    )

    # Phase 2: Analyze each paper
    analyses = []
    for paper in papers:
        analysis = analyze_paper(paper)
        analyses.append(analysis)

    # Phase 3: Generate report
    report = create_report(analyses)

    # Update session with success
    client.sessions.update(
        is_successful=True,
        is_successful_reason=f"Successfully analyzed {len(papers)} papers"
    )

    print(f"Report generated: {report[:200]}...")
```

## Next Steps

- Learn about [Session Management](/python-sdk/init)
- Explore [Event Management](/python-sdk/events/index)
- Understand [Decorators](/python-sdk/decorators)
- Set up [Custom Rubrics](/feature-overview/rubrics)
- Use the [Prompt Database](/feature-overview/prompt-db)
