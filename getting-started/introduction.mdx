---
title: Introduction
description: Get started with understanding how and why our agent observability platform works.
---

# ðŸ‘‹ Welcome to Your Agent's Control Room

AI agents donâ€™t just make predictions â€” they perform **multi-step workflows**, invoke tools, navigate interfaces, and adapt in real-time.

That complexity comes at a cost: itâ€™s easy for agents to behave unexpectedly, fail silently, or succeed for the wrong reasons.

We built this platform to give you **deep observability** into how your agent thinks, acts, and learns â€” across every run.

---

## ðŸ§  Why Observability Matters

Running an agent once doesn't tell you how it behaves.

- Agents are **non-deterministic**
- They're made of **chained actions**, not just outputs
- A single successful session can hide dozens of edge cases

To build agents you can trust, you need to see what they're doing â€” and why.

---

## ðŸ›  What This Platform Does

We help you:

- Analyze **individual Sessions** of agent behavior
- Inspect each **Step** and the **Events** within
- Run **Mass Simulations** to observe variability at scale
- Visualize agent behavior through **Workflow Trajectories**
- Define structured, flexible **Rubric Evaluations**
- Version and experiment with prompts via [Prompt DB](/prompt-db)
- (Soon) Compare simulation results with [Compare Sims](/compare-sims)
- (Soon) Debug stuck logic using [Time Travel](/time-travel)

---

## ðŸ“¦ Core Concepts

Start here to understand the building blocks:

- [Sessions](/sessions) â€“ A full run of your agent performing a task
- [Steps](/steps) â€“ Each state + action in the session timeline
- [Events](/events) â€“ Low-level actions like LLM or tool calls
- [Mass Simulations](/mass-simulations) â€“ Many sessions, one task
- [Workflow Trajectory](/features/workflow-trajectory) â€“ Merged graph of all agent paths
- [Rubric Evaluation](/rubric-evaluation) â€“ Your opinionated scoring system

<Callout type="info">
Prefer high-level context first? Check out our [Design Principles](/core-concepts/principles).
</Callout>

---

## ðŸš€ Next Steps

Ready to dive in?

- **[Get Started in the UI](/getting-started)** â€” Run your first Session and view the dashboard
- **[Use the Python SDK](/sdk/overview)** â€” Record and evaluate agents from your code
- **[Define Your First Rubric](/rubric-evaluation)** â€” Create custom evaluation logic for your use case

Weâ€™re excited to help you build agents that are **observable**, **de-buggable**, and **production-ready**.

---

## ðŸ’¬ Need Help?

Questions, feedback, or something not working right?

Reach out to us at [support@yourdomain.ai](mailto:support@yourdomain.ai) â€” weâ€™d love to help.
```

---