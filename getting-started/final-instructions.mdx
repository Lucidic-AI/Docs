---
title: Code Setup (Part 2)
description: Learn how to install LucidicAI and set up Steps, Events, and Sessions.
---

> **Note:** Setup has two parts â€” (1) connecting it to the [Lucidic Dashboard](/getting-started/dashboard), and (2) [installing LucidicAI and setting up steps, events, and sessions in your code](/getting-started/final-instructions) (below).

## Install the SDK

Our package is published on PyPI under the name `lucidicai`.

```bash
pip install lucidicai
```

We recommend installing it into a **virtual environment** or as part of your existing agent stack.

### Minimum Requirements

- Python **3.8+**
- Compatible with all major LLM frameworks and agent libraries
- Works in local, server, or cloud environments

---

# Using the Python SDK

Once you've [installed Lucidic](/getting-started/final-instructions#ðŸ“¦-install-the-sdk) and [connected it to the dashboard](/getting-started/dashboard), you're ready to start recording your agent's behavior, step by step.

This guide walks you through how to initialize a session, track steps and events, and optionally pull prompts from our [Prompt DB](/features/prompt-db).

---

## Step 1: Create a Session

Creating a session requires the `init` function to be called on the `lai` object: `lai.init()`. 

The most important parameters are:
- `session_name` - A descriptive name for your session
- `providers` - List of LLM providers to track (e.g., `["openai"]`, `["anthropic"]`)
- `task` - What your agent is trying to accomplish

For a complete list of all parameters and detailed documentation, see the [`init` API reference](/api-reference/init).

### Setting up Authentication

You need to provide your Lucidic API key and Agent ID. You can do this in two ways:

**Option 1: Environment Variables (Recommended)**
```bash
# In your .env file or terminal
export LUCIDIC_API_KEY=your-api-key
export LUCIDIC_AGENT_ID=your-agent-id
```

Then initialize with just the providers:
```python
import lucidicai as lai

lai.init(
    session_name="Search Wikipedia",
    providers=["openai"],
    task="Search Wikipedia for Steve Jobs biography"
)
```

**Option 2: Direct Parameters**
```python
import lucidicai as lai

lai.init(
    session_name="Search Wikipedia",
    lucidic_api_key="your-api-key",   
    agent_id="your-agent-id",         
    providers=["openai"],
    task="Search Wikipedia for Steve Jobs biography"
)
```

---

## Step 2: Create Steps

Each step should represent a meaningful action or decision point. 

Use `lai.create_step()` and `lai.end_step()` around your step logic:
- `lai.create_step()` - Creates a step and attaches information to it
- `lai.end_step()` - Ends the step and optionally updates its information

Key parameters include:
- `state` - Current environment or context
- `action` - What the agent is doing
- `goal` - What the agent aims to achieve
- `eval_score` - Performance rating
- `screenshot_path` - Visual context (optional)

For complete parameter documentation, see the [`create_step`](/api-reference/create_step) and [`end_step`](/api-reference/end_step) API reference. 

```python
lai.create_step(
    state="Home page", 
    action="Navigating to search",
    goal="Navigate to search",
    screenshot_path="/path/to/screenshot.png",
    eval_score=5,
    eval_description="Agent successfully navigated to search page",
) # All fields are optional

# Your agent logic here...

# All fields will be overwritten if they are provided in end_step
lai.end_step(
    action="Clicked search",
    eval_score=4,
    eval_description="Agent successfully clicked search",
)
```

> Only **one step can be active at a time**, so be sure to end a step before creating a new one.

### Automatic Step Creation

If you're using a supported provider (OpenAI, Anthropic, LangChain, PydanticAI) and make an LLM call without an active step, Lucidic will **automatically create one for you**:

```python
import lucidicai as lai
from openai import OpenAI

lai.init(providers=["openai"])

client = OpenAI()

# No step created yet, but this still works!
# Lucidic auto-creates a step when the LLM call happens
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)

# The auto-created step will be ended when the session ends
```

This ensures you never lose tracking data, even if you forget to create steps explicitly.

---

### (Optional) Step Updates

You can update the current step mid-run using `lai.update_step()`. 

The parameters are the exact same as `lai.create_step`. You would use this if you want to add/adjust any information after a step has been created.

```python
lai.create_step(
    ...
)

...

lai.update_step(
    state="Updated state",
    action="Updated action",
    goal="Updated goal",
    eval_score=5,
    eval_description="Updated eval description",
    # Use either screenshot OR screenshot_path, not both
    screenshot_path="/path/to/image.png"
)

...

lai.end_step(
    ...
)
```

---

## Step 3: Create Events

If your agent has tool calls or LLM usage, track them as **events** within each step. 
> If you only have LLM calls with a supported provider you don't need to create events for them! Lucidic will automatically create events for you. For instance, if you set `providers=["openai"]` in `lai.init()`, Lucidic will automatically create events for you for any OpenAI calls.

Key parameters for events include:
- `description` - What the event is doing
- `result` - Output or response from the event
- `model` - LLM model name (if applicable)
- `cost_added` - Cost of this event
- `screenshots` - Visual context

For complete parameter documentation, see the [`create_event`](/api-reference/create_event) and [`end_event`](/api-reference/end_event) API reference.

> **Note:** All fields will be overwritten EXCEPT for screenshots and cost_added which are appended/added.

```python
lai.create_event(
    description="OpenAI call",
    result="Summary generated",
    model="gpt-4o",
    cost_added=0.01,
    screenshots=["/path/to/screenshot.png"]
)
# Do your LLM call...

# All fields will be overwritten EXCEPT for screenshots and cost_added which are appended/added
lai.end_event(cost_added=0.01, screenshots=["/path/to/screenshot1.png", "/path/to/screenshot2.png"]) 
```

> **Reminder:** LLM calls from supported providers like OpenAI and Anthropic are **automatically captured** into events when you set the `providers` parameter in `init()`.

---

## Step 4: End the Session

### Automatic Step Ending

When your session ends (either manually or automatically), Lucidic will **automatically end any active steps** for you. This means:

- You don't need to worry about forgetting to call `lai.end_step()`
- All steps are properly closed before the session completes
- Your data remains consistent even if your script exits unexpectedly

```python
lai.create_step(state="Processing", goal="Analyze data")
# Do some work...
# Oops, forgot to end the step!

# No problem - when session ends, the step is auto-ended
lai.end_session()  # Or let it auto-end on script exit
```

### Automatic Session Ending

Your session will automatically end when the script exits (via an `atexit` handler), but you can also end it manually with `lai.end_session` if you would like to add evaluations:

```python
lai.end_session(
    is_successful=True,
    is_successful_reason="Completed all steps",
    session_eval=5,
    session_eval_reason="Accurate and efficient"
)
```

**All fields are optional â€” if you don't provide them, Lucidic will automatically evaluate the session for you.**

For complete parameter documentation, see the [`end_session` API reference](/api-reference/end_session).

### How Auto-End Works

The `atexit` handler ensures your session data is always captured:

```python
import lucidicai as lai
from openai import OpenAI

lai.init(providers=["openai"])

client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Script ends here - no explicit end_session() needed!
# Lucidic automatically:
# 1. Ends any active steps
# 2. Ends the session
# 3. Sends all data to the dashboard
```

This means you can instrument existing scripts with just 2 lines of code!

### Additional Information: Custom Evaluation Rubrics

When ending a session, you can provide your own evaluation metrics to better assess your agent's performance:

- If you don't provide evaluation metrics, Lucidic will automatically evaluate your session using default rubrics.
- If you provide custom evaluation metrics, Lucidic will use your metrics instead of the default ones.

```python
# When ending the session, provide your custom evaluation
custom_evaluation, custom_evaluation_reason = custom_evaluation(...)

lai.end_session(
    is_successful=True,
    session_eval=custom_evaluation,
    session_eval_reason=custom_evaluation_reason
)
``` 

We recommend creating your own rubric for custom tasks to ensure the evaluation aligns with your specific requirements:

```python
# When initializing your session, specify custom rubrics
lai.init(
    session_name="custom_task",
    providers=["openai", "anthropic"],
    rubrics=["default", "my_custom_rubric"]  # Include both default and custom rubrics
)
```

Custom rubrics allow you to:
- Define task-specific success criteria
- Implement domain-specific evaluation metrics
- Compare agent performance across different versions using consistent metrics
- Track improvements over time with metrics tailored to your use case

Learn more about creating and using custom rubrics in the [Evaluation Rubrics](/features/rubrics) documentation.

---

### Why Both `is_successful` and `session_eval`?

We support two types of evaluations so you can keep things simple or add more detail:

* `is_successful`: A binary flag â€” did the agent complete the task?
* `session_eval`: A more detailed score (e.g. 1â€“5) showing *how well* the agent did.

If you only want to pass in one value (like a numeric score), you can compute the other yourself:

```python
score = 5
lai.end_session(
    is_successful=score >= 4,
    session_eval=str(score),
)
```

This gives you full control and lets you keep your logic consistent with your own success criteria.

---

## Step 5: (Optional) Pull Prompts from Prompt DB

Use Lucidic's [Prompt DB](/features/prompt-db) so you never have to go to your code to make a prompt change again!

Pull prompts from our prompt database in your code, change the content in the dashboard, and see them automatically update on your next agent run! We support prompt versioning and labels so you can have different versions of a prompt for different use cases (e.g. production, testing, etc.).

```python
prompt = lai.get_prompt(
    prompt_name="summarize_webpage",
    variables={"page_text": "Stanford University is..."},
    label="production",
    cache_ttl=60
)
```

For complete parameter documentation, see the [`get_prompt` API reference](/api-reference/get_prompt) and the [Prompt DB feature guide](/features/prompt-db).

Additional notes:

- `{{variable}}` placeholders will be replaced using your dictionary
- Missing keys will raise an error
- Unreplaced variables will trigger a warning
- Read more at [Prompt DB](/features/prompt-db)

---

## Full Session Code Example

```python
import lucidicai as lai

lai.init(
    session_name="wiki_search",
    agent_id="agent-123",
    providers=["openai"],
    task="Search for Stanford University",
    rubrics=["default"]
)

lai.create_step(state="Home", goal="Search for Stanford")

# (Your logic here)

# EVENT 1
lai.create_event(description="OpenAI call")
# (Your LLM call)
lai.end_event(result="Went to Wikipedia") # Optional 

# EVENT 2
lai.create_event(description="OpenAI call")
# (Your LLM call)
lai.end_event(result="Found Stanford University") # Optional 

lai.end_step(action="Used search", eval_score=5, eval_description="Found Stanford University") # Optional

lai.end_session(is_successful=True) # Optional 
```

---

## Minimal Instrumentation Example

Thanks to Lucidic's automatic features, you can instrument existing workflows with just **2 lines of code**:

```python
import lucidicai as lai
lai.init(providers=["openai"])  # Or ["anthropic"], ["langchain"], etc.

# That's it! Your existing code now has full observability
```

Here's a complete example showing how minimal the setup can be:

```python
import lucidicai as lai
from openai import OpenAI

# Just these 2 lines to add complete observability!
lai.init(providers=["openai"])

# Your existing code remains unchanged
client = OpenAI()

def analyze_text(text):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"Analyze this: {text}"}]
    )
    return response.choices[0].message.content

def summarize_text(text):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"Summarize this: {text}"}]
    )
    return response.choices[0].message.content

# Your workflow runs normally
text = "The quick brown fox jumps over the lazy dog."
analysis = analyze_text(text)
summary = summarize_text(analysis)
print(summary)

# When script exits, Lucidic automatically:
# 1. Creates steps for each distinct operation
# 2. Creates events for each LLM call
# 3. Ends all steps properly
# 4. Ends the session
# 5. Sends everything to your dashboard
```

### What Happens Automatically:

1. **Session Creation**: A session starts when you call `init()`
2. **Step Creation**: Steps are auto-created when LLM calls happen
3. **Event Creation**: Every LLM call becomes an event automatically
4. **Step Ending**: Active steps are auto-ended when the session ends
5. **Session Ending**: The session auto-ends when your script exits
6. **Data Upload**: All tracking data is sent to your dashboard

This means you can add Lucidic to any existing Python script that uses supported LLM providers and immediately get full visibility into your AI workflows!

---

## Next Steps

- [Evaluate Sessions with Rubrics](/features/rubrics)
- [Run Mass Simulations](/features/mass-simulation-overview)
- [View Session Replays in the Workflow Sandbox](/features/workflow-sandbox)

---