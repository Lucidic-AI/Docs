---
title: Quick Start
description: Get complete AI observability with just a few lines of code
---

> **Note:** Setup has two parts â€” (1) connecting it to the [Lucidic Dashboard](/getting-started/dashboard), and (2) installing LucidicAI in your code (below).

## The Magic of Drop-in Observability

With just a few lines of code, get complete visibility into your AI workflows:

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])

with client.sessions.create(session_name="My Task"):
    # All LLM calls are automatically tracked!
    response = openai.chat.completions.create(...)
```

## What Happens Automatically

When you use Lucidic:

- **Every LLM call is tracked** - Automatic event creation for all API calls
- **Sessions end gracefully** - Automatic cleanup on context exit or script end
- **All data flows to your dashboard** - Real-time visibility
- **Works with existing code** - Minimal refactoring required

## Installation

```bash
pip install lucidicai
```

Set your credentials (get them from the [dashboard](/getting-started/dashboard)):
```bash
export LUCIDIC_API_KEY=your-api-key
export LUCIDIC_AGENT_ID=your-agent-id
```

## Complete Example

Here's a real-world example showing how minimal the setup can be:

```python
from lucidicai import LucidicAI
from openai import OpenAI

# Initialize Lucidic client
client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

def analyze_text(text):
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"Analyze this: {text}"}]
    )
    return response.choices[0].message.content

def summarize_text(text):
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"Summarize this: {text}"}]
    )
    return response.choices[0].message.content

# Use context manager for automatic session management
with client.sessions.create(session_name="Text Analysis"):
    text = "The quick brown fox jumps over the lazy dog."
    analysis = analyze_text(text)
    summary = summarize_text(analysis)
    print(summary)

# When context exits, Lucidic automatically:
# 1. Creates events for each LLM call
# 2. Ends the session
# 3. Sends everything to your dashboard
```

## Supported Providers

Just change the provider name when initializing:

```python
# OpenAI
client = LucidicAI(api_key="...", providers=["openai"])

# Anthropic
client = LucidicAI(api_key="...", providers=["anthropic"])

# Multiple providers
client = LucidicAI(api_key="...", providers=["openai", "anthropic"])

# Other supported providers
client = LucidicAI(api_key="...", providers=["langchain"])
client = LucidicAI(api_key="...", providers=["pydantic_ai"])
client = LucidicAI(api_key="...", providers=["openai_agents"])
```

## Want More Control?

While the basic setup handles everything automatically, you can take control when needed:

<CardGroup cols={2}>
  <Card title="Use Decorators" icon="at" href="/python-sdk/events/decorator">
    Track function calls with @client.event() decorator
  </Card>
  <Card title="Session Management" icon="play" href="/python-sdk/init">
    Configure sessions with custom names, IDs, and metadata
  </Card>
  <Card title="Manual Events" icon="bolt" href="/python-sdk/events/create">
    Create custom events for any operation
  </Card>
  <Card title="Zero-Latency Emit" icon="forward" href="/python-sdk/events/emit">
    Track events without blocking your code
  </Card>
</CardGroup>

<Card title="Full Examples" icon="code" href="/python-sdk/examples">
  See complete examples with manual control and decorators
</Card>

## Next Steps

<CardGroup cols={2}>
  <Card title="View Your Sessions" icon="chart-line" href="/feature-overview/workflow-sandbox">
    See your tracked sessions in the dashboard
  </Card>
  <Card title="Add Evaluations" icon="star" href="/feature-overview/rubrics">
    Score and evaluate your agent's performance
  </Card>
  <Card title="Run Experiments" icon="flask" href="/feature-overview/experiments">
    Group sessions for bulk analysis
  </Card>
  <Card title="Explore Integrations" icon="plug" href="/integrations/openai">
    Learn about provider-specific features
  </Card>
</CardGroup>

---

## Minimum Requirements

- Python **3.8+**
- Compatible with all major LLM frameworks
- Works in local, server, or cloud environments

That's it! You now have complete observability into your AI workflows.
