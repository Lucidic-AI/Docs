---
title: Anthropic Integration
description: Automatically track Claude completions in your agent sessions with a single line of setup.
---

# Anthropic Integration

Lucidic supports automatic tracking of **Anthropic Claude** completions in your agent sessions — with no extra instrumentation needed.

---

## How It Works

When you initialize with:

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["anthropic"])
```

Lucidic will:

- **Instrument** the Anthropic client using OpenTelemetry
- **Automatically create events** for each Claude completion
- Track events within the currently active **Session**
- Support both sync and async operations

---

## What Gets Captured

We automatically capture the following from Anthropic API calls:

- **Input**: your messages/prompt to Claude
- **Model**: the Claude model used (e.g. `claude-3-opus-20240229`, `claude-3-5-sonnet-20241022`)
- **Output**: the Claude response (including streaming)
- **Token usage**: input and output tokens
- **Cost**: calculated based on token usage and model pricing
- **Timing**: duration of the API call
- **Images**: when using vision capabilities


---

## Why This Matters

LLM calls are a core part of most agent workflows — but without visibility, it's impossible to debug or optimize:

- Which call caused the **failure**?
- Which **session** was it part of?
- How much did it **cost**?
- What was the actual **response**?

This integration gives you full transparency, with **zero boilerplate**.

---

## Example

```python
from lucidicai import LucidicAI
from anthropic import Anthropic

# Initialize clients
client = LucidicAI(api_key="...", providers=["anthropic"])
anthropic_client = Anthropic()

# Use context manager for session lifecycle
with client.sessions.create(session_name="claude_task"):
    # LLM calls are automatically tracked as events
    response = anthropic_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=500,
        messages=[{"role": "user", "content": "Hello Claude!"}]
    )
# Session auto-ends when context exits
```

### Streaming Example

```python
from lucidicai import LucidicAI
from anthropic import Anthropic

client = LucidicAI(api_key="...", providers=["anthropic"])
anthropic_client = Anthropic()

with client.sessions.create(session_name="streaming_demo"):
    # Streaming responses are also tracked automatically
    stream = anthropic_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1000,
        messages=[{"role": "user", "content": "Explain quantum computing"}],
        stream=True
    )

    for event in stream:
        if event.type == "content_block_delta":
            print(event.delta.text, end="")
```

### With Function Decorators

```python
from lucidicai import LucidicAI
from anthropic import Anthropic

client = LucidicAI(api_key="...", providers=["anthropic"])
anthropic_client = Anthropic()

@client.event()
def generate_ideas(topic: str) -> str:
    """Generate research ideas - tracked as a Function Call event."""
    response = anthropic_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=500,
        messages=[{"role": "user", "content": f"What are 5 research ideas about {topic}?"}]
    )
    return response.content[0].text

@client.event()
def evaluate_ideas(ideas: str) -> str:
    """Evaluate ideas - tracked as a Function Call event."""
    response = anthropic_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=500,
        messages=[{"role": "user", "content": f"Evaluate these ideas: {ideas}"}]
    )
    return response.content[0].text

# Run the workflow
with client.sessions.create(session_name="research_session"):
    ideas = generate_ideas("climate change")
    evaluation = evaluate_ideas(ideas)
    print(evaluation)

# Event hierarchy:
# Session
# └── Event: generate_ideas (Function Call)
#     └── Event: Claude call (LLM Generation)
# └── Event: evaluate_ideas (Function Call)
#     └── Event: Claude call (LLM Generation)
```

---

## Notes

- Requires `anthropic` Python SDK installed (`pip install anthropic`)
- All Anthropic client methods are instrumented
- Both sync and async methods are supported
- Events are automatically nested within decorated functions
- You can still manually use `client.events.emit()` for additional context
- Works with the latest Anthropic Python SDK

---

## See Also

- [Event Management](/python-sdk/events/index) - Overview of events
- [Decorators](/python-sdk/decorators) - Automatic function tracking
- [Examples](/python-sdk/examples) - More usage patterns
