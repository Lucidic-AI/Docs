---
title: OpenAI Integration
description: Automatically capture OpenAI LLM calls in your sessions with one line of setup.
---

# OpenAI Integration

Lucidic makes it easy to **automatically track OpenAI completions** as part of your agent's behavior — no code changes required.

---

## How It Works

When you initialize with:

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])
```

Lucidic will:

- **Instrument** the OpenAI client using OpenTelemetry
- Automatically log each LLM call as an **Event**
- Track events within the currently active **Session**

This means:

- **Automatic Event Creation**: Every OpenAI API call is automatically captured as an event
- You get **full observability into prompt, model, cost, result, and more — out of the box**
- Works with both sync and async OpenAI client methods

---

## What Gets Captured

We automatically capture the following from OpenAI API calls:

- **Input**: your messages/prompt to OpenAI
- **Model**: the model name (e.g. `gpt-4`, `gpt-5.2`, `gpt-4o`)
- **Output**: OpenAI response (including streaming responses)
- **Token usage**: input and output tokens
- **Cost**: calculated based on token usage and model pricing
- **Timing**: duration of the API call
- **Images**: when using vision models

---

## Why This Matters

LLM calls are a core part of most agent workflows — but without visibility, it's impossible to debug or optimize:

- Which call caused the **failure**?
- Which **session** was it part of?
- How much did it **cost**?
- What was the actual **response**?

This integration gives you full transparency, with **zero boilerplate**.

---

## Example

```python
from lucidicai import LucidicAI
from openai import OpenAI

# Initialize clients
client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

# Use context manager for session lifecycle
with client.sessions.create(session_name="chatbot_run"):
    # LLM calls are automatically tracked as events
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Hello!"}]
    )
# Session auto-ends when context exits
```

### Streaming Example

```python
from lucidicai import LucidicAI
from openai import OpenAI

client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

with client.sessions.create(session_name="streaming_demo"):
    # Streaming responses are also tracked automatically
    stream = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Tell me a story"}],
        stream=True
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="")
```

### With Function Decorators

```python
from lucidicai import LucidicAI
from openai import OpenAI

client = LucidicAI(api_key="...", providers=["openai"])
openai_client = OpenAI()

@client.event()
def research_questions(topic: str) -> str:
    """Generate research questions - tracked as a Function Call event."""
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"What are key areas in {topic} research?"}]
    )
    return response.choices[0].message.content

@client.event()
def analyze_challenges(questions: str) -> str:
    """Analyze challenges - tracked as a Function Call event."""
    response = openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"What are the main challenges in each area? {questions}"}]
    )
    return response.choices[0].message.content

# Run the workflow
with client.sessions.create(session_name="research_session"):
    questions = research_questions("AI safety")
    analysis = analyze_challenges(questions)
    print(analysis)

# Event hierarchy:
# Session
# └── Event: research_questions (Function Call)
#     └── Event: GPT-4 call (LLM Generation)
# └── Event: analyze_challenges (Function Call)
#     └── Event: GPT-4 call (LLM Generation)
```

---

## Notes

- All OpenAI client methods are instrumented (chat completions, embeddings, etc.)
- Both sync and async methods are supported
- Events are automatically nested within decorated functions
- You can always add custom events using `client.events.emit()` for additional context
- Works with the latest OpenAI Python SDK (v1.0+)

---

## See Also

- [Event Management](/python-sdk/events/index) - Overview of events
- [Decorators](/python-sdk/decorators) - Automatic function tracking
- [Examples](/python-sdk/examples) - More usage patterns
