---
title: Browser Use Integration
description: Use our pre-integrated fork of Browser Use to track browser agent sessions with full Lucidic observability.
---

# ğŸŒ Browser Use Integration

[Browser Use](https://github.com/openaccess-ai-collective/browser-agent) is a popular open-source **browser agent** built on LangChain. Weâ€™ve created a **Lucidic-integrated fork** so you can track and debug your browser agent workflows with full session replay, step timelines, event logging, and prompt evaluation â€” right out of the box.

---

## ğŸš€ What Weâ€™ve Added

In our fork of the Browser Use agent:

- Lucidic calls like `lai.create_step()` and `lai.end_step()` are automatically inserted into the agent logic
- LLM and tool interactions are tracked as **Events**
- Support for Mass Sims, Prompt DB, and Rubric evaluations is built in
- Works with LangChain handler for full call graph coverage

---

## ğŸ§­ How to Use It

### 1. Clone the Lucidic Fork

```bash
git clone https://github.com/lucidic-ai/browser-use-lai.git
cd browser-use
```

> ğŸ“˜ This is a fork of the official repo with added observability.`

---

### 2. Install Locally

```bash
pip install . ."[dev]"
```

This installs the agent in editable mode so you can tweak it if needed.

---

### 3. Use the LangChain Handler

Since Browser Use is built on LangChain, you still need to attach the Lucidic handler manually.

In your script:

```python
import lucidicai as lai
from browser_use.agent import BrowserAgent

# Initialize Lucidic
lai.init(
    session_name="browser_run",
    agent_id="your-agent-id",
    provider="langchain"
)

# Create agent and attach LangChain handler
agent = BrowserAgent()
handler = lai.LucidicLangchainHandler()
handler.attach_to_llms(agent)

# Run as usual
agent.run("Find the capital of Japan and report its population.")
```

---

### 4. Example Script

You can run full-scale testing of your browser agents using our [example script on GitHub](https://github.com/Lucidic-AI/browser-use-lai/blob/main/examples/models/test.py). It supports multiple models, prompt versions, and tasks â€” all fully tracked with Lucidic.

Below are a few key snippets:

---

#### ğŸ” Test Multiple Models

Loop through LLMs like GPT-4 and Gemini using a shared browser agent:

```python
llm_configs = [
  {"name": "gpt-4.1-nano", "provider": "openai", "api_key": openai_key},
  {"name": "gemini-2.0-flash", "provider": "google", "api_key": gemini_key},
]

for llm in llm_configs:
    agent = Agent(llm=load_llm(llm), task="Search Hello world on google", browser=shared_browser)
    await agent.run()
```

---

#### ğŸ§  Test Different Prompt Versions

Dynamically inject different versions of prompts (`production`, `development`, etc.) into your agents:

```python
prompt_labels = ["production", "development"]

for label in prompt_labels:
    content = lai.get_prompt("System Prompt", label=label)
    system_message = SystemMessage(content=content.format(max_actions=4))
```

---

#### ğŸ§ª Run Mass Simulations

Track each run under a different session name and mass sim ID:

```python
lai.init(
  task="Search Hello world on google",
  session_name=f"{prompt_label} | {llm_name}",
  mass_sim_id="a89c61f9-1a16-4a68-85f3-31cd2eca8abc"
)
```

---

## ğŸ”— Links

- ğŸ”— [Lucidic Fork of Browser Use](https://github.com/lucidic-ai/browser-use-lai)
- ğŸ“˜ [LangChain Integration](/integrations/langchain)
- âš™ï¸ [Using the Python SDK](/getting-started/python)
- ğŸ”¬ [Workflow Sandbox](/features/workflow-sandbox)

---