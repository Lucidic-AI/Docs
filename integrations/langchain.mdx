---
title: LangChain Integration
description: Automatically track LangChain LLM calls by attaching Lucidic to your agent — even if it's deeply nested.
---

# LangChain Integration

Lucidic integrates seamlessly with **LangChain** agents and chains — no need to rewrite your code or restructure your logic.

---

## How It Works

When you use `providers=["langchain"]` with Lucidic:

- LangChain LLM calls are **automatically instrumented** using OpenTelemetry
- **Events are automatically created** for every LLM call
- No need to manually attach handlers in most cases
- All LLM usage is captured as **Lucidic Events**, grouped within the current Session

This means you get **full observability into your agent's reasoning — with minimal setup.**

---

## Example

```python
from lucidicai import LucidicAI
from langchain_openai import ChatOpenAI

# Initialize clients
client = LucidicAI(api_key="...", providers=["langchain"])

llm = ChatOpenAI(model="gpt-4")

# Use context manager for session lifecycle
with client.sessions.create(session_name="langchain_demo"):
    # LLM calls are automatically tracked as events
    response = llm.invoke("Hello!")
# Session auto-ends when context exits
```

### Streaming Example

```python
from lucidicai import LucidicAI
from langchain_openai import ChatOpenAI

client = LucidicAI(api_key="...", providers=["langchain"])

llm = ChatOpenAI(model="gpt-4")

with client.sessions.create(session_name="streaming_demo"):
    # Streaming responses are also tracked automatically
    for chunk in llm.stream("Tell me a story"):
        print(chunk.content, end="")
```

### With Function Decorators

```python
from lucidicai import LucidicAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

client = LucidicAI(api_key="...", providers=["langchain"])

llm = ChatOpenAI(model="gpt-4")

@client.event()
def generate_questions(topic: str) -> str:
    """Generate research questions - tracked as a Function Call event."""
    response = llm.invoke([
        SystemMessage(content="You are a research assistant."),
        HumanMessage(content=f"What are 5 important areas in {topic}?")
    ])
    return response.content

@client.event()
def analyze_methodology(questions: str) -> str:
    """Analyze methodology - tracked as a Function Call event."""
    response = llm.invoke([
        HumanMessage(content=f"What methodologies are used to study each area? {questions}")
    ])
    return response.content

# Run the workflow
with client.sessions.create(session_name="research_session"):
    questions = generate_questions("climate science")
    analysis = analyze_methodology(questions)
    print(analysis)

# Event hierarchy:
# Session
# └── Event: generate_questions (Function Call)
#     └── Event: GPT-4 call (LLM Generation)
# └── Event: analyze_methodology (Function Call)
#     └── Event: GPT-4 call (LLM Generation)
```

---

## What Gets Captured

For every LangChain LLM call, we automatically capture:

- **Input**: your messages/prompt to the LLM
- **Model**: the LLM model used (e.g. `gpt-4`, `claude-3`)
- **Output**: the LLM response (including streaming)
- **Token usage**: input and output tokens
- **Cost**: calculated based on token usage and model pricing
- **Chain context**: which chain or agent made the call
- **Tool usage**: any tools invoked by the agent


---

## Agent Example

```python
from lucidicai import LucidicAI
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import Tool
from langchain_core.prompts import PromptTemplate

client = LucidicAI(api_key="...", providers=["langchain"])

# Define tools
tools = [
    Tool(
        name="Calculator",
        func=lambda x: eval(x),
        description="Useful for math calculations"
    )
]

# Create agent
llm = ChatOpenAI(model="gpt-4")
prompt = PromptTemplate.from_template("You are a helpful assistant. {input}")
agent = create_react_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools)

with client.sessions.create(session_name="langchain_agent"):
    # All LLM calls within the agent are automatically tracked!
    result = executor.invoke({"input": "What is 25 * 4 + 10?"})
    print(result)
```

### Complex Agent Handler (Optional)

For some complex agent setups, you may need to manually attach a handler:

```python
from lucidicai.telemetry.otel_handlers import OTelLangChainHandler

# Get the handler from the SDK
handler = client.get_provider("langchain")

# Manually attach to your agent if needed
if hasattr(agent, 'callbacks'):
    agent.callbacks.append(handler)
```

In most cases, automatic instrumentation will work without manual attachment.

---

## Notes

- Requires LangChain packages installed (`langchain`, `langchain-openai`, etc.)
- Works with all LangChain LLMs, chains, and agents
- Automatic instrumentation handles most use cases
- Both sync and async operations are supported
- Tool calls and chain execution are tracked as separate events

---

## See Also

- [Event Management](/python-sdk/events/index) - Overview of events
- [Decorators](/python-sdk/decorators) - Automatic function tracking
- [Examples](/python-sdk/examples) - More usage patterns
