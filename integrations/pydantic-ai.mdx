---
title: PydanticAI Integration
description: Automatically capture PydanticAI agent completions in your sessions with one line of setup.
---

# PydanticAI Integration

Lucidic makes it easy to **automatically track PydanticAI agent completions** as part of your agent's behavior — no code changes required.

---

## How It Works

When you initialize with:

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["pydantic_ai"])
```

Lucidic will:

- **Instrument** PydanticAI's model classes using OpenTelemetry
- **Automatically create events** for each agent call
- Track events within the currently active **Session**
- Support all PydanticAI model types (OpenAI, Anthropic, Gemini, Groq, etc.)

This means:

- You get full observability into prompts, model, cost, result, and more — out of the box
- No manual event creation needed

---

## What Gets Captured

Each PydanticAI call is tracked with:

- `description`: formatted messages sent to the model
- `model`: the underlying model name (e.g. `claude-3-5-sonnet-20241022`, `gpt-4`)
- `cost`: calculated based on token usage
- `result`: truncated summary of the response


---

## Why This Matters

PydanticAI agent calls are a core part of most AI workflows — but without visibility, it's impossible to debug or optimize:

- Which call caused the failure?
- Which session was it part of?
- How much did it cost?
- What was the actual response?

This integration gives you full transparency, with **zero boilerplate**.

---

## Example

```python
from lucidicai import LucidicAI
from pydantic_ai import Agent
import asyncio

# Initialize clients
client = LucidicAI(api_key="...", providers=["pydantic_ai"])

agent = Agent()  # Uses default model

async def run():
    with client.sessions.create(session_name="pydantic_ai_demo"):
        # LLM calls are automatically tracked as events
        response = await agent.run("Hello!")
        return response

asyncio.run(run())
# Session auto-ends when context exits
```

### Streaming Example

```python
from lucidicai import LucidicAI
from pydantic_ai import Agent
import asyncio

client = LucidicAI(api_key="...", providers=["pydantic_ai"])

agent = Agent()

async def stream_example():
    with client.sessions.create(session_name="streaming_demo"):
        # Streaming responses are also tracked automatically
        async with agent.run_stream("Tell me a story") as response:
            async for text in response.stream():
                print(text, end="")

asyncio.run(stream_example())
```

### With Custom Model

```python
from lucidicai import LucidicAI
from pydantic_ai import Agent
from pydantic_ai.models.anthropic import AnthropicModel
import asyncio

client = LucidicAI(api_key="...", providers=["pydantic_ai"])

model = AnthropicModel("claude-3-5-sonnet-20241022")
agent = Agent(model=model, system_prompt="You are a research assistant.")

@client.event()
async def generate_ideas(topic: str) -> str:
    """Generate research ideas - tracked as a Function Call event."""
    response = await agent.run(f"What are 5 research ideas about {topic}?")
    return response.data

@client.event()
async def analyze_methodology(ideas: str) -> str:
    """Analyze methodology - tracked as a Function Call event."""
    response = await agent.run(f"What methodologies could be used for each idea? {ideas}")
    return response.data

async def research():
    with client.sessions.create(session_name="research_session"):
        ideas = await generate_ideas("climate change")
        analysis = await analyze_methodology(ideas)
        return analysis

result = asyncio.run(research())
print(result)

# Event hierarchy:
# Session
# └── Event: generate_ideas (Function Call)
#     └── Event: Claude call (LLM Generation)
# └── Event: analyze_methodology (Function Call)
#     └── Event: Claude call (LLM Generation)
```

---

## Notes

- All PydanticAI model types are instrumented (OpenAI, Anthropic, Gemini, Groq, etc.)
- Both sync and async agent runs are supported
- Events are automatically nested within decorated functions
- Streaming responses are tracked automatically
- You can always add custom events using `client.events.emit()` for additional context

---

## See Also

- [Event Management](/python-sdk/events/index) - Overview of events
- [Decorators](/python-sdk/decorators) - Automatic function tracking
- [Examples](/python-sdk/examples) - More usage patterns
