---
title: Rubrics
description: Build flexible, structured evaluation rubrics to score and compare your agent's behavior across sessions.
---

# 📏 Rubrics

**Rubrics** let you define structured evaluation criteria for your agents — turning your domain knowledge into consistent, explainable scoring systems.

Instead of relying on a single pass/fail outcome or ad hoc scores, rubrics help you **quantify agent quality** across dimensions you care about.

---

## ⚙️ How to Use Rubrics

Usiing rubrics consists of 2 parts, first creating one in the UI and then using it in your code.

### Step 1: In the UI

- Navigate to the **Rubrics** tab on any Agent
  > 📷 _![rubric](/images/r2.png)_
  > 📷 _![rubric](/images/r3.png)_
  > (Right underneath the agent name, so in the example right below "Browseruse Demo")
- Click create rubric button
  > 📷 _![rubric](/images/r4.png)_
  > 📷 _![rubric](/images/r5.png)_
- Create, name, and configure a rubric
  > 📷 _![rubric](/images/r6.png)_
- Click Save
- Click on the rubric name on the top bar to see a summary of the rubric
  > 📷 _![rubric](/images/r7.png)_

### Step 2: In Python

During initialization, you can attach rubrics to a session by name.

```python
lai.init(
    session_name="custom_task",
    provider="openai",
    rubrics=["default", "my_custom_rubric"]  # Include both default and custom rubrics
)
```

---

## 🧠 Why Create Custom Rubrics?

Agents don’t just succeed or fail — they:

- They have small task specificities (i.e is the resturant open, is the resturant closed)
- They have certain paths which are more efficient than others (clicking finding the menu on the resturants website vs trying to find the menu by looking on door dash)
-

Rubrics let you **capture those nuances** and **standardize how you evaluate** an agent’s session. They’re perfect for comparing sessions, identifying regressions, or measuring improvement across prompt or model changes.

### Default Rubric

Lucidic has a default rubric which is attached to every session. You can find it in the UI and it is called "default". It is a basic rubric which tries to determine how good was the agent at completing the task.
The criterias are:

- Task Completion (40%)
- Attention to Detail (25%)
- Efficiency (20%)
- Error Recovery (10%)
- Safety (5%)

If you are interested in how we made the default rubric, you can find it in the UI and it is called "default". In general, it is supposed to be a good starting point for most use cases.

---

## 🧬 Rubric Anatomy

A rubric contains:

- One or more **Criteria**
- Optional **weights** for each criterion
- **Score Definitions** (what a score of 1 to 10 means for each criterion)

### Example

```yaml
Rubric: "Browser Navigation Quality"

Criteria:
  - name: "Revisiting Sites"
    weight: 2
    scores:
      10: "Never visited a site more than once"
      5: "Occasionally revisited"
      1: "Frequently revisited the same sites"

  - name: "Step Efficiency"
    weight: 1
    scores:
      10: "Completed task in under 10 steps"
      5: "11–15 steps"
      1: "More than 15 steps"
```

---

## 📊 Scoring and Weights

Each criterion is scored independently (typically from 1–5) and then combined into a **composite score**:

- Scores range from a minimum of 1 to a maximum of 10
- Criteria serve as "goalposts" - you only need to define key reference points (e.g., what scores of 1, 5, and 10 mean) rather than every possible score
- The system will interpolate intermediate scores based on the agentic context captured through steps and events

### Weighting Calculations

- If no weights are defined, all criteria are weighted equally
- If weights are set, we compute a **weighted average** using their relative values:
  - Weights can be expressed as percentages or relative values
  - All weights are summed and each criterion's score is multiplied by its proportion of the total weight
  - The final score is the sum of these weighted criterion scores

For example, with criteria A (weight 2) scoring 8, and criteria B (weight 1) scoring 4:

- Total weight: 3
- Weighted score: ((8 × 2) + (4 × 1)) ÷ 3 = 6.67


#### Score Interpretation

- Decimal scores are rounded to two decimal places for display purposes
- A criterion with no explicitly defined score for a certain level will use the nearest defined reference point
- When an agent's performance falls between two defined score points, the system interpolates linearly

#### Weight Formats

- Weights can be expressed as integers (e.g., 1, 2, 3) or percentages (e.g., 20%, 30%, 50%)
- When using percentages, they should sum to 100% for clarity, though the system will normalize them if they don't
- Zero weights are allowed but not recommended—consider removing the criterion instead

#### Minimum Required Criteria

- A rubric must have at least two criteria to be valid a minimum and maximum. So this means if you specify a 1 and a 2, we will not interpolate to 5 or 10 and assume 2 is the best score that can be achieved.
- For meaningful comparative analysis, we recommend at least three criteria

#### Score Distributions

- The scoring system supports non-linear distributions if needed
- You can define more reference points for areas where you need finer granularity
- Example: You could define scores for 1, 2, 3, 7, and 10 if the middle range isn't important to your evaluation

#### Programmatic Access

- All rubric scores are available via the API for integration with your own analytics
- Historical scoring data is preserved even if rubric definitions change

---

## 🔮 What's Next

Coming soon:

- Rubric analytics across Mass Sims
- Graph overlays colored by rubric score
- Rubric diffing across prompt or model changes
- Export/import rubrics across teams
- LLM-assisted rubric scoring

---
