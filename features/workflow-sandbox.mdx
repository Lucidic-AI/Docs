---
title:  Viewing Sessions (Workflow Sandbox)
description: Dive into a full breakdown of one agent run â€” replay, inspect, evaluate, and debug every decision.
---

# ðŸ§ª  Workflow Sandbox

The **Workflow Sandbox** is your deep-dive view into a single agent run â€” also known as a [Session](/core-concepts/sessions). Learn to create a session [here](/getting-started/final-instructions).

Itâ€™s where observability meets usability: you can **replay the session**, **inspect every step**, **explore the underlying events**, and **evaluate performance** using rubrics â€” all in one place.

---

## ðŸ§  What It Shows
> To get here Click Project -> Agent -> any session in the session history.


At the top of the Workflow Sandbox, youâ€™ll see:

### 1. ðŸ“ Task Overview
- The task string supplied to the agent at the start of the session

### 2. ðŸ“Š Session Stats
- **Session Evaluation** - Your overall session evaluation
- **Is Successful** - Whether the session was successful or not
- **Cost** - The cost of the session
- **Time** - The time taken to complete the session
- **Total Steps** - The total number of steps taken in the session
- **Repeated Steps** - The number of repeated steps in the session
- **Average Steps per node** - The average number of steps per node in the session (this is a proxy for how many times the agent retried a step)
> ðŸ“· _![api key2](/images/ws1.png)_

---
>ðŸ§  **Pro tip:** Hover over the session evaluation or the success/failure flag to see a the description!! 


## ðŸ§® Rubric Evaluations

Below the overview, youâ€™ll find any evaluation rubrics attached to this session. Read more about rubrics [here](/features/rubrics).

- Each criterion is scored and explained
- The **Lucidic default rubric** runs on every session automatically
- Custom rubrics can be attached per Agent when running a session

>ðŸ§  **Pro tip:**: The description of why you got a certain score for a given criterion is below the score, so you can actually see why each of your criteria got a certain score! 

> ðŸ“· _![rubric](/images/ws2.png)_

---

## ðŸ“š Quick Access: Prompt DB

You can open the [Prompt DB](/features/prompt-db) directly from the session:

- Quickly view which prompt was used
- Test changes or view version history
- Add tags to quickly iterate on prompts



---
## ðŸ§­ Session Replay: Step Trajectory Graph

Next, youâ€™ll see a **Session-specific graph** â€” a mini version of the [Workflow Trajectory](/features/workflow-trajectory), scoped to just this one session.

- **Nodes** = states visited
- **Edges** = actions taken
- **Grouped** = similar states are clustered (e.g. looping or retrying)

>ðŸ§  **Pro tip:**: Clicking a node opens a full breakdown of that **Step** below the graph!

> ðŸ“· _![trajectory](/images/ws3.png)_


>ðŸ§  **Pro tip:**: The step which caused your agent to fail will be highlighted in red!

> ðŸ“· _![trajectory](/images/ws6.png)_
---



## ðŸ•° Timeline Navigation

Two synced timelines appear below:

### ðŸªœ Step Timeline
- Shows the ordered sequence of Steps
- Clicking a step:
  - Advances the graph to that state
  - Opens the step detail panel
- Step detail panel shows:
  - Basic Information: Start time, duration, Cost
  - Evaluation: Status, step eval score, eval description
  - State overview: Goal, Action, State
  - Image of state

> ðŸ“· _![step detail](/images/ws4.png)_

>ðŸ§  **Pro tip:**: Use arrow keys to quickly go from one step to the next.

### âš™ï¸ Event Timeline (within Step)
- Shows all [Events](/core-concepts/events) that occurred in that step (e.g. LLM calls, tool invocations)
- Clicking an event shows:
  - Request/response
  - Images used (if any)
  - Cost
  - Model used
  - Any errors or retries
  - Time travels (read more about them [here](/features/time-travel))
  - Pretty vs raw request/response

> ðŸ“· _![event timeline](/images/ws5.png)_

> ðŸ§  **Pro tip:**: If your LLM calls are super cluttered, use our pretty text which is built in! It's the wand and will show you your text in a readable format. 

---

## ðŸ§° Use Cases

The Workflow Sandbox is where you:

- Debug a failed run
- Investigate edge cases
- Replay odd behavior
- Evaluate agent consistency
- Identify root causes of errors
- Understand how a prompt or model change affected logic

---

## ðŸ”® Future Features

- Annotate steps or events
- Inline prompt diffing
- Attach notes to runs for team reviews or QA


---