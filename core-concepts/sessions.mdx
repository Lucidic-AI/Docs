---
title: Sessions
description: Understand the lifecycle, structure, and analysis of a Session – a single run of your AI agent.
---

# Sessions

A **Session** represents a single run of an AI agent as it attempts to complete a task. It captures the full trace of the agent’s behavior — from initial state to final outcome — and provides deep visibility into each decision, action, and outcome along the way.

---

## What is a Session?

A Session is a top-level unit of observability. It begins when an agent starts a task and ends when the task is either completed, failed, or interrupted. 
Learn to create a session with our [Quick Start guide](/getting-started/quickstart) or see [detailed examples](/python-sdk/examples)

Within each Session UI in Lucidic or what we call [workflow sandbox](/feature-overview/workflow-sandbox), you'll find:

- A **timeline of Events** (LLM calls, function calls, and custom operations)
- **Nested event hierarchy** showing how operations relate to each other
- A **workflow graph** visualizing the session's trajectory
- A **GIF/video replay** of the agent's interaction (if available)
- Evaluation scores, cost metrics, metadata, and more

<Note type="info">
Sessions are non-deterministic by nature. Re-running the same task may lead to different outcomes due to the agent’s autonomy and stochasticity.
</Note>

---

## Session Structure

### Event Hierarchy

A Session contains **Events**, which represent individual operations within your agent workflow. Events can be nested to show hierarchical relationships between operations.

```
Session
├── Event (LLM Generation) ← auto-created via OpenTelemetry
├── Event (Function Call) ← @client.event() decorator
│   └── Event (LLM Generation) ← nested, auto-created
├── Event (Error Traceback) ← auto-created on exception
└── Event (Generic) ← client.events.create() / .emit()
```

**Event Types:**
- **LLM Generation** - Automatically captured API calls to LLM providers
- **Function Call** - Function executions tracked via decorators
- **Error Traceback** - Exceptions with full stack traces
- **Generic Event** - Any custom operation you want to track

<Note type="note">
Events are documented in detail [here →](/feature-overview/events).
</Note>

---

## Session Dashboard or Workflow Sandbox

Each Session in the [workflow sandbox](/feature-overview/workflow-sandbox) includes:

- **Overview metrics**: duration, event count, cost, completion status
- **Visual replay**: full session video or GIF (if captured)
- **Interactive workflow graph**: clickable nodes for each event
- **Event explorer**: deep dive into inputs, outputs, and nested events
- **Evaluation scores**: human or LLM-generated



---

## Evaluation and Scoring

Sessions can be evaluated via multiple systems:

### 1. **User-Provided Eval Score**
Passed in at runtime using the SDK. Useful for customized or domain-specific scoring.

```python
from lucidicai import LucidicAI

client = LucidicAI(api_key="...", providers=["openai"])

with client.sessions.create(session_name="My Task"):
    # Your workflow here
    pass

# Or end with evaluation details
client.sessions.end(
    is_successful=True,
    is_successful_reason="Agent completed primary objectives but took longer than expected"
)
```

### 2. **Rubric-Based Evaluation**
Define structured, weighted rubrics with multiple criteria and score definitions.

```yaml
criteria:
  - name: "Repeated Site Visits"
    weights: 0.4
    scores:
      10: "Never visited a site twice"
      1: "Frequently revisited pages"

  - name: "Average Step Time"
    weight: 0.6
    ...
```

These rubrics produce both per-criterion and composite scores.

### 3. **Default Evaluation**
Our built-in system outputs:
- A score (0 or 1) based on task completion
- A list of "wrong" or suboptimal events (automatically identified)
- Graph highlights of failure points

---

## Debugging with Sessions

Sessions give you rich insight into how your agent operates and where it goes wrong:

- **Replay stuck behavior** via video
- **Identify high-cost operations**
- **Trace decision-making** back to specific prompts or events
- **Group failing events** into unified nodes in the graph
- **Track revisited states** and looping patterns


---

## Metadata and Tagging

Each Session contains rich metadata including:

- Task description
- Agent identity and version
- Cost
- Timestamps
- Experiment ID (if part of an experiment)
- Replay data and graph reference

Planned: **semantic tagging**, **search**, and **session filtering** based on LLM-labeled themes.

---

## Related Concepts

- [Events](/feature-overview/events) - Individual operations within sessions
- [Experiments](/feature-overview/experiments) - Grouping sessions for analysis
- [Rubric Evaluation](/feature-overview/rubrics) - Structured evaluation criteria
- [Workflow Sandbox](/feature-overview/workflow-sandbox) - Session visualization

---