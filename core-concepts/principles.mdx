---
title: Design Principles
description: The core philosophies that guide how we think about agent observability, evaluation, and debugging.
---

import Callout from '@components/callout'

# ğŸ§± Core Principles

Our platform is built around a simple truth:

> Running an agent once is never enough.

AI agents are non-deterministic, multi-step, and often fragile systems. Without deep observability and structured evaluation across many runs, you're essentially flying blind. These are the principles that shape every feature we build.

---

## ğŸ² 1. Embrace Non-Determinism

AI agents donâ€™t behave the same way every time â€” and thatâ€™s not a bug, itâ€™s a feature. But it also means:

- One successful run doesnâ€™t prove robustness
- Subtle bugs can hide behind â€œluckyâ€ completions
- Behavior must be understood **probabilistically**, not just procedurally

Thatâ€™s why we built [Mass Simulations](/mass-simulations) â€” to give you a **statistical view** of what your agent *usually* does, not just what it *did once*.

---

## ğŸª 2. Observability Is Mandatory

Agents arenâ€™t just outputs â€” theyâ€™re **workflows**.

Understanding how an agent moves through a task requires full-stack observability:

- [Sessions](/sessions): what happened on one run?
- [Steps](/steps): what state was the agent in at each moment?
- [Events](/events): what low-level operations occurred?
- [Workflow Trajectory](/features/workflow-trajectory): what paths does the agent take across many runs?

We treat agents like complex systems â€” and we give you the tools to inspect them like one.

---

## ğŸ“‰ 3. Reliability Is a Spectrum

There is no binary â€œworkingâ€ or â€œnot workingâ€ in agents. There is only:

- How often does it succeed?
- How does it fail when it fails?
- What factors make it more or less likely to succeed?

We donâ€™t just show outcomes. We show **failure modes**, **step evaluations**, and **decision graphs** so you can debug with nuance â€” not guesses.

---

## ğŸ“ 4. Let the Builder Be Opinionated

You know your agent better than anyone.

Thatâ€™s why we support **custom evaluation pipelines** like:

- Manual [Eval Scores](/sessions#user-provided-eval-score)
- [Rubrics](/sessions#rubric-based-evaluation) with weighted criteria
- Optional prompt versioning with [Prompt DB](/prompt-db)

Our job isnâ€™t to force a one-size-fits-all judgment system. Itâ€™s to give you flexible tools for measuring what *you* care about.

---

## ğŸ§ª 5. Agent Testing Should Be Like Software Testing

In traditional software:

- You donâ€™t deploy after one test
- You write many test cases
- You monitor behavior over time

Agent dev should be the same.

Weâ€™re building the testing framework that agents deserve â€” complete with bulk simulations, failure analytics, prompt iteration tools, and side-by-side comparisons (coming soon with [Compare Sims](/compare-sims)).

<Callout type="info">
The future of AI isnâ€™t just better models. Itâ€™s better debugging, iteration, and evaluation tooling.
</Callout>

---

## ğŸ”­ What Weâ€™re Building Toward

Our north star is full **agent analytics** â€” the ability to:

- Run structured simulations
- Explore decision trajectories
- Evaluate agents across rubrics and real-world behaviors
- Identify where and why agents fail
- Deploy with confidence

We believe that **transparency** and **repeatability** are the missing pieces to unlocking truly production-grade AI agents.

---

## ğŸ”— Related Concepts

- [Mass Simulations](/mass-simulations)
- [Workflow Trajectory](/features/workflow-trajectory)
- [Sessions](/sessions)
- [Steps](/steps)
- [Prompt DB](/prompt-db)
```

---