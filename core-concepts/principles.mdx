---
title: Design Principles
description: The core philosophies that guide how we think about agent observability, evaluation, and debugging.
---

import Callout from '@components/callout'

# 🧱 Core Principles

Our platform is built around a simple truth:

> Running an agent once is never enough.

AI agents are non-deterministic, multi-step, and often fragile systems. Without deep observability and structured evaluation across many runs, you're essentially flying blind. These are the principles that shape every feature we build.

---

## 🎲 1. Embrace Non-Determinism

AI agents don’t behave the same way every time — and that’s not a bug, it’s a feature. But it also means:

- One successful run doesn’t prove robustness
- Subtle bugs can hide behind “lucky” completions
- Behavior must be understood **probabilistically**, not just procedurally

That’s why we built [Mass Simulations](/mass-simulations) — to give you a **statistical view** of what your agent *usually* does, not just what it *did once*.

---

## 🪞 2. Observability Is Mandatory

Agents aren’t just outputs — they’re **workflows**.

Understanding how an agent moves through a task requires full-stack observability:

- [Sessions](/sessions): what happened on one run?
- [Steps](/steps): what state was the agent in at each moment?
- [Events](/events): what low-level operations occurred?
- [Workflow Trajectory](/features/workflow-trajectory): what paths does the agent take across many runs?

We treat agents like complex systems — and we give you the tools to inspect them like one.

---

## 📉 3. Reliability Is a Spectrum

There is no binary “working” or “not working” in agents. There is only:

- How often does it succeed?
- How does it fail when it fails?
- What factors make it more or less likely to succeed?

We don’t just show outcomes. We show **failure modes**, **step evaluations**, and **decision graphs** so you can debug with nuance — not guesses.

---

## 📏 4. Let the Builder Be Opinionated

You know your agent better than anyone.

That’s why we support **custom evaluation pipelines** like:

- Manual [Eval Scores](/sessions#user-provided-eval-score)
- [Rubrics](/sessions#rubric-based-evaluation) with weighted criteria
- Optional prompt versioning with [Prompt DB](/prompt-db)

Our job isn’t to force a one-size-fits-all judgment system. It’s to give you flexible tools for measuring what *you* care about.

---

## 🧪 5. Agent Testing Should Be Like Software Testing

In traditional software:

- You don’t deploy after one test
- You write many test cases
- You monitor behavior over time

Agent dev should be the same.

We’re building the testing framework that agents deserve — complete with bulk simulations, failure analytics, prompt iteration tools, and side-by-side comparisons (coming soon with [Compare Sims](/compare-sims)).

<Callout type="info">
The future of AI isn’t just better models. It’s better debugging, iteration, and evaluation tooling.
</Callout>

---

## 🔭 What We’re Building Toward

Our north star is full **agent analytics** — the ability to:

- Run structured simulations
- Explore decision trajectories
- Evaluate agents across rubrics and real-world behaviors
- Identify where and why agents fail
- Deploy with confidence

We believe that **transparency** and **repeatability** are the missing pieces to unlocking truly production-grade AI agents.

---

## 🔗 Related Concepts

- [Mass Simulations](/mass-simulations)
- [Workflow Trajectory](/features/workflow-trajectory)
- [Sessions](/sessions)
- [Steps](/steps)
- [Prompt DB](/prompt-db)
```

---