---
title: Steps
description: Learn how Steps represent agent states and transitions within a Session.
---

import Callout from '/snippets/callout.mdx'

# 🪜 Steps

A **Step** is a unit of execution within a [Session](/core-concepts/sessions). It captures both a snapshot of the agent’s **current state** and the **action taken** to transition to the next state. Steps are the backbone of a session timeline and form the nodes in your workflow graph.

Each Step may contain multiple underlying [Events](/core-concepts/events), such as tool calls, API requests, or LLM invocations.

---

## 🧬 Step Anatomy

Each Step records:

- The agent’s **state** (optional string + LLM-generated summary)
- The **goal** the agent is trying to accomplish (optional string + summary)
- The **action** taken to transition to the next step (with optional LLM-generated summary)
- **Timestamps** for start and update time
- **Cost** for operations within this step
- Whether the step was **finished**, **successful**, or **stuck**
- Whether a **screenshot** was captured
- An **evaluation score + description** if provided or computed
---

## 🧭 State, Goal, and Action

- **State**: where the agent is right now (e.g., a page it's on, a field it’s viewing, or a task state)
- **Goal**: what the agent wants to do in this step
- **Action**: how the agent attempts to reach the goal

If provided, you can supply these fields manually through your agent SDK. Otherwise, summaries can be automatically generated via LLMs if raw text is available.


---

## 🎬 Screenshots and Visual State

Steps may include a screenshot that visually captures what the agent was seeing or interacting with at that moment. These can be critical for understanding UI-based agents or browser actions.

- Displayed in the Step detail view
- Used for default evaluations and visual debugging
- Stored per-step and accessible via the SDK or dashboard

---

## 💥 Evaluating Steps

Steps can be scored individually using:

- **User-provided scores**: log your own logic-based rating of a step
- **LLM-generated scores** (planned): use limited context to judge a step's quality
- **Default heuristics**: highlight stuck or repeated states

```python
lai.update_step(eval_score=0.3, eval_description="Tried to switch tabs but failed.")
```

<Callout type="warning">
Step-level evaluation in isolation is often difficult without broader task context. We recommend pairing this with session-wide scoring or rubrics when possible.
</Callout>

---

## 🔄 Step Transitions & Workflow Graph

Steps link together to form the execution path of your agent:

```text
[state₀] --(action)--> [state₁] --(action)--> [state₂] ...
```

In your dashboard’s **Workflow Graph**, Steps are:

- Represented as **nodes**
- Clustered automatically when the state is repeated (e.g. looping or retries)
- Connected by labeled edges that represent the agent’s actions


---

## 🔍 Grouping & Graph Insight

When multiple Steps share the same state and similar actions (e.g., retries or confusion loops), they’re automatically grouped in the graph view. This helps surface:

- Stuck behavior (e.g., trying the same thing repeatedly)
- Revisiting states (e.g., reloading a site, flipping between tabs)
- Structural patterns (e.g., loops, branches, dead ends)

---

## 📁 Events Inside Steps

Each Step may contain multiple [Events](/events) — fine-grained units of work including:

- Tool invocations
- LLM calls
- API requests
- DOM scraping
- etc.


---

## 🧪 Debugging with Steps

Steps are the atomic unit of debugging:

- Replay exactly what the agent saw or did
- Inspect the agent’s thought process (via goal + action summaries)
- View logs and evaluations
- Freeze at any step and "time travel" with new prompts (coming soon)

---

## ✨ Coming Soon

- Per-step prompt replays
- Visual comparison across step variations
- Semantic annotations + auto-tagging
- LLM-based per-step eval suggestions

---
